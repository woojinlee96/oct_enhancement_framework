import torch.optim as optim
import torch.utils.data
import torch.nn as nn
from torch.autograd import Variable
from tqdm import tqdm
import torchvision

import pandas as pd
import pytorch_ssim
import pytorch_msssim
from loss import GeneratorLoss
from model import Generator, Discriminator
import utils
import data_loader
import time
import os
from torch.optim.lr_scheduler import ReduceLROnPlateau

from tensorboardX import SummaryWriter

model_name = '211021'

utils.get_clear_tensorboard()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
writer = SummaryWriter(logdir="log_tensorboard/")

'''Dataset setting'''
path = '/home/leewj/projects/projects/oct_enhancement/f_space/dataset'
trainloader = data_loader.get_loader(dataset_path=path, phase='train', shuffle=True, batch_size=8, num_workers=2)
valloader = data_loader.get_loader(dataset_path=path, phase='val', shuffle=True, batch_size=8, num_workers=2)

netG = Generator()
netD = Discriminator()

generator_criterion = GeneratorLoss()
discriminator_criterion = nn.BCELoss()

netG = nn.DataParallel(netG, device_ids=[0], output_device=0)
netD = nn.DataParallel(netD, device_ids=[0], output_device=0)

netG.to(device)
netD.to(device)
generator_criterion.to(device)
discriminator_criterion.to(device)

optimizerG = optim.Adam(netG.parameters(), lr=0.001, weight_decay=5e-4)
optimizerD = optim.Adam(netD.parameters(), lr=0.0001)

schedulerG = ReduceLROnPlateau(optimizerG, mode='min', factor=0.1, patience=300, min_lr = 0.0001, verbose=False)

results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'msssim': [], 'ssim': [],
           'g_loss_mse' : [], 'g_loss_l1' : [], 'g_loss_grad': [], 'g_loss_ssim' : [],  'g_loss_ad': [],
           'd_loss_real': [], 'd_loss_fake': []}

NUM_EPOCHS = 8000

data, target = next(iter(trainloader))
utils.imshow(torchvision.utils.make_grid(torch.cat((data[0:6, :], target[0:6, :]), dim=0), nrow=3))

Gcheckpoint = torch.load('output/netG_%s.pth' % (model_name))
netG.load_state_dict(Gcheckpoint['State_dict'])
optimizerG.load_state_dict(Gcheckpoint['optimizer'])
#
Dcheckpoint = torch.load('output/netD_%s.pth' % (model_name))
netD.load_state_dict(Dcheckpoint['State_dict'])
optimizerD.load_state_dict(Dcheckpoint['optimizer'])
Epoch = Dcheckpoint['Epoch']

for epoch in range(Epoch, NUM_EPOCHS + 1):
    train_bar = tqdm(trainloader)
    running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}
    generator_loss = {'MSE' : 0, 'L1' : 0, 'SSIM_loss' : 0, 'GRAD' : 0, 'TV' : 0, 'Adversarial' : 0}
    discriminator_loss = {'REAL' : 0, 'FAKE' : 0}

    netG.train()
    netD.train()
    tic = time.time()
    with torch.autograd.set_detect_anomaly(True):
        for data, target in train_bar:
            g_update_first = True
            data, target = data.to(device), target.to(device)
            batch_size = data.size(0)
            running_results['batch_sizes'] += batch_size

            real_labels = torch.ones((target.size(0), 1)).to(device)
            fake_labels = torch.zeros((target.size(0), 1)).to(device)

            real_img = Variable(target)
            z = Variable(data)


            # Discriminator update
            netD.zero_grad()
            netG.zero_grad()
            fake_img = netG(z)
            real_out = netD(real_img)
            fake_out = netD(fake_img)

            d_loss_real = discriminator_criterion(real_out, real_labels)
            d_loss_fake = discriminator_criterion(fake_out, fake_labels)

            d_loss = d_loss_real + d_loss_fake

            if epoch < 600:
                if epoch%30 == 0 or epoch == 1:
                    d_loss.backward(retain_graph=True)
                    optimizerD.step()


            # Generator update
            netD.zero_grad()
            netG.zero_grad()

            fake_img = netG(z)
            fake_out = netD(fake_img)

            adversarial_loss = discriminator_criterion(fake_out, real_labels)
            mse_loss, l1_loss, s_loss, grad_loss, tv_loss = generator_criterion(fake_img, real_img)

            # g_loss = mse_loss * 0.3 + l1_loss * 0.7 + s_loss + grad_loss * 0.9 + tv_loss * 2e-6
            # g_loss = mse_loss * 0.3 + l1_loss * 0.7 + s_loss * 0 + grad_loss * 0.9 + tv_loss * 0
            g_loss = mse_loss * 0.6 + l1_loss * 0.8 + s_loss + grad_loss * 0.9 + tv_loss * 0;
            g_loss += adversarial_loss*0.0001
            g_loss.backward()


            optimizerG.step()

            # loss for current batch before optimization
            running_results['g_loss'] += g_loss.item() * batch_size
            running_results['d_loss'] += d_loss.item() * batch_size
            running_results['d_score'] += real_out.mean().item() * batch_size
            running_results['g_score'] += fake_out.mean().item() * batch_size

            generator_loss['MSE'] += (mse_loss*0.3).item() * batch_size
            generator_loss['L1'] += (l1_loss*0.7).item() * batch_size
            generator_loss['SSIM_loss'] += s_loss.mean().item() * batch_size
            generator_loss['GRAD'] += (grad_loss*0.9).mean().item() * batch_size
            generator_loss['TV'] += (tv_loss*2e-6).mean().item() * batch_size
            generator_loss['Adversarial'] += (adversarial_loss*0.0001).mean().item() * batch_size

            discriminator_loss['REAL'] += d_loss_real.mean().item() * batch_size
            discriminator_loss['FAKE'] += d_loss_fake.mean().item() * batch_size

            train_bar.set_description(
                desc='[%d/%d] Loss_D:%.3f Loss_G:%.3f D(x):%.2f D(G(z)):%.2f, Time:%.2f s, LR:%.5f' % (
                epoch, NUM_EPOCHS, running_results['d_loss'] / running_results['batch_sizes'],
                running_results['g_loss'] / running_results['batch_sizes'],
                running_results['d_score'] / running_results['batch_sizes'],
                running_results['g_score'] / running_results['batch_sizes'],
                time.time() - tic,
                optimizerG.param_groups[0]['lr'])
            )


        if epoch % 100 == 0 or epoch == 1:
            result_show = torchvision.utils.make_grid(
                (torch.cat((data[0:3, :], target[0:3, :], fake_img[0:3, :]), dim=0)), nrow=3)
            utils.imshow(result_show.cpu().squeeze(), title=[str(epoch) + 'epoch : result'])

    schedulerG.step(g_loss)

    netG.eval()
    with torch.no_grad():
        val_bar = tqdm(valloader)
        valing_results = {'mse': 0, 'ssims': 0, 'msssims':0,'msssim': 0, 'ssim': 0, 'batch_sizes': 0}
        val_images = []
        for data, target  in val_bar:
            batch_size = data.size(0)
            valing_results['batch_sizes'] += batch_size

            data, target = data.to(device), target.to(device)

            real_img = Variable(target)
            fake_img = netG(data)

            batch_mse = ((fake_img - real_img) ** 2).data.mean()
            valing_results['mse'] += batch_mse * batch_size

            batch_ssim = pytorch_ssim.ssim(fake_img, real_img).item()
            valing_results['ssims'] += batch_ssim * batch_size

            batch_msssim = pytorch_msssim.msssim(fake_img, real_img, normalize=True).item()
            valing_results['msssims'] += batch_msssim * batch_size

            valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']
            valing_results['msssim'] = valing_results['msssims'] / valing_results['batch_sizes']
            val_bar.set_description(
                desc='[Validating results] MSSSIM: %.4f SSIM: %.4f' % (
                    valing_results['msssim'], valing_results['ssim']))

    # save loss\scores\psnr\ssim
    results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])
    results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])
    results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])
    results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])
    results['msssim'].append(valing_results['msssim'])
    results['ssim'].append(valing_results['ssim'])

    results['g_loss_mse'].append(generator_loss['MSE'] / running_results['batch_sizes'])
    results['g_loss_l1'].append(generator_loss['L1'] / running_results['batch_sizes'])
    results['g_loss_ssim'].append(generator_loss['SSIM_loss'] / running_results['batch_sizes'])
    results['g_loss_grad'].append(generator_loss['GRAD'] / running_results['batch_sizes'])
    results['g_loss_ad'].append(generator_loss['Adversarial'] / running_results['batch_sizes'])

    results['d_loss_real'].append(discriminator_loss['REAL'] / running_results['batch_sizes'])
    results['d_loss_fake'].append(discriminator_loss['FAKE'] / running_results['batch_sizes'])

    writer.add_scalar('Loss/Loss G', running_results['g_loss'] / running_results['batch_sizes'], epoch)
    writer.add_scalar('Loss/Loss D', running_results['d_loss'] / running_results['batch_sizes'], epoch)

    writer.add_scalar('GENERATOR/MSE', generator_loss['MSE'] / running_results['batch_sizes'], epoch)
    writer.add_scalar('GENERATOR/L1', generator_loss['L1'] / running_results['batch_sizes'], epoch)
    writer.add_scalar('GENERATOR/SSIM', generator_loss['SSIM_loss'] / running_results['batch_sizes'], epoch)
    writer.add_scalar('GENERATOR/GRAD', generator_loss['GRAD'] / running_results['batch_sizes'], epoch)
    writer.add_scalar('GENERATOR/TV', generator_loss['TV'] / running_results['batch_sizes'], epoch)
    writer.add_scalar('GENERATOR/Adversarial', generator_loss['Adversarial'] / running_results['batch_sizes'], epoch)

    writer.add_scalar('DISCRIMINATOR/RAEL', discriminator_loss['REAL'] / running_results['batch_sizes'], epoch)
    writer.add_scalar('DISCRIMINATOR/FAKE', discriminator_loss['FAKE'] / running_results['batch_sizes'], epoch)

    writer.add_scalar('Score/D(x)', running_results['d_score'] / running_results['batch_sizes'], epoch)
    writer.add_scalar('Score/D(G(z))', running_results['g_score'] / running_results['batch_sizes'], epoch)

    writer.add_scalar('ValingResults/msssim', valing_results['msssim'], epoch)
    writer.add_scalar('ValingResults/ssim', valing_results['ssim'], epoch)

    writer.add_scalar('learning rate', optimizerG.param_groups[0]['lr'], epoch)

    # save model parameters
    if epoch > 1:
        utils.save_checkpoint(epoch, netG, optimizerG, 'output/netG_%s.pth' % (model_name))
        utils.save_checkpoint(epoch, netD, optimizerD, 'output/netD_%s.pth' % (model_name))

        # data_frame = pd.DataFrame(
        #     data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],
        #           'Score_G': results['g_score'], 'g_loss_mse' : results['g_loss_mse'], 'g_loss_l1' : results['g_loss_l1'],
        #           'g_loss_ssim' : results['g_loss_ssim'], 'g_loss_grad' : results['g_loss_grad'], 'g_loss_ad' : results['g_loss_ad'],
        #           'd_loss_real' : results['d_loss_real'], 'd_loss_fake' : results['d_loss_fake']},
        #     index=range(1, epoch + 1))

        data_frame = pd.DataFrame(
            data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],
                  'Score_G': results['g_score'], 'g_loss_mse' : results['g_loss_mse'], 'g_loss_l1' : results['g_loss_l1'],
                  'g_loss_ssim' : results['g_loss_ssim'], 'g_loss_grad' : results['g_loss_grad'], 'g_loss_ad' : results['g_loss_ad'],
                  'd_loss_real' : results['d_loss_real'], 'd_loss_fake' : results['d_loss_fake']},
            index=range(Epoch, epoch + 1))

        if not os.path.exists('statistics/%s' % (model_name)):
            os.makedirs('statistics/%s' % (model_name))

        data_frame.to_csv('statistics/%s/%d_train_results.csv' % (model_name, epoch), index_label='Epoch')

writer.close()